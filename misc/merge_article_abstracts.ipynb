{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining paths\n",
    "data_file = 'kaggle/arxiv_metadata_2020_to_2023.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001.00001</td>\n",
       "      <td>mathematics can help analyze the arts and in...</td>\n",
       "      <td>quantum gestart: identifying and applying corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001.00002</td>\n",
       "      <td>in the paper we provide some polynomial iden...</td>\n",
       "      <td>on identities of $2$-dimensional algebras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001.00003</td>\n",
       "      <td>word embedding is an essential building bloc...</td>\n",
       "      <td>learning numeral embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001.00004</td>\n",
       "      <td>online algorithm has been an emerging area o...</td>\n",
       "      <td>new competitive analysis results of online lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001.00005</td>\n",
       "      <td>the objective of this paper is to construct ...</td>\n",
       "      <td>approach to the construction of the spaces $ s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           abstract  \\\n",
       "0  2001.00001    mathematics can help analyze the arts and in...   \n",
       "1  2001.00002    in the paper we provide some polynomial iden...   \n",
       "2  2001.00003    word embedding is an essential building bloc...   \n",
       "3  2001.00004    online algorithm has been an emerging area o...   \n",
       "4  2001.00005    the objective of this paper is to construct ...   \n",
       "\n",
       "                                               title  \n",
       "0  quantum gestart: identifying and applying corr...  \n",
       "1          on identities of $2$-dimensional algebras  \n",
       "2                        learning numeral embeddings  \n",
       "3  new competitive analysis results of online lis...  \n",
       "4  approach to the construction of the spaces $ s...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the dataframe into memory\n",
    "metadata_df = pd.read_parquet(data_file)\n",
    "\n",
    "## Print the first 5 rows\n",
    "(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataset dataframe\n",
    "dataset_df = pd.DataFrame(columns=['id', 'title', 'abstract', 'article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00002v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00008v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00007v2.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00007v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00006v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00001v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00006v2.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00004v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00003v1.txt',\n",
       " 'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00005v1.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ds_txt_files = glob('tmp_ds_extracted_article/**/*.txt', recursive=True)\n",
    "tmp_ds_txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2301.00002'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_name = tmp_ds_txt_files[0].split('/')[-1].split('.')[0] + '.' + tmp_ds_txt_files[0].split('/')[-1].split('.')[1]\n",
    "pdf_name.split('v')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  we present experimental results to explore a form of bivariate glyphs for\\nrepresenting large-magnitude-range vectors. the glyphs meet two conditions: (1)\\ntwo visual dimensions are separable; and (2) one of the two visual dimensions\\nuses a categorical representation (e.g., a categorical colormap). we evaluate\\nhow much these two conditions determine the bivariate glyphs' effectiveness.\\nthe first experiment asks participants to perform three local tasks requiring\\nreading no more than two glyphs. the second experiment scales up the search\\nspace in global tasks when participants must look at the entire scene of\\nhundreds of vector glyphs to get an answer. our results support that the first\\ncondition is necessary for local tasks when a few items are compared. but it is\\nnot enough to understand a large amount of data. the second condition is\\nnecessary for perceiving global structures of examining very complex datasets.\\nparticipants' comments reveal that the categorical features in the bivariate\\nglyphs trigger emergent optimal viewers' behaviors. this work contributes to\\nperceptually accurate glyph representations for revealing patterns from large\\nscientific results. we release source code, quantum physics data, training\\ndocuments, participants' answers, and statistical analyses for reproducible\\nscience https://osf.io/4xcf5/?view_only=94123139df9c4ac984a1e0df811cd580.\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Search extracted id in the meadata_df\n",
    "metadata = metadata_df[metadata_df['id'] == pdf_name[:-2]]\n",
    "metadata['abstract'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2301.00002</td>\n",
       "      <td>evaluating alternative glyph design for showin...</td>\n",
       "      <td>we present experimental results to explore a...</td>\n",
       "      <td>introduction section for\\nsearch of \"largest\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2301.00002</td>\n",
       "      <td>evaluating alternative glyph design for showin...</td>\n",
       "      <td>we present experimental results to explore a...</td>\n",
       "      <td>introduction section for\\nsearch of \"largest\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2301.00002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we present experimental results to explore a...</td>\n",
       "      <td>introduction section for\\nsearch of \"largest\" ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  2301.00002  evaluating alternative glyph design for showin...   \n",
       "1  2301.00002  evaluating alternative glyph design for showin...   \n",
       "2  2301.00002                                                NaN   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    we present experimental results to explore a...   \n",
       "1    we present experimental results to explore a...   \n",
       "2    we present experimental results to explore a...   \n",
       "\n",
       "                                             article  \n",
       "0  introduction section for\\nsearch of \"largest\" ...  \n",
       "1  introduction section for\\nsearch of \"largest\" ...  \n",
       "2  introduction section for\\nsearch of \"largest\" ...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the extracted article\n",
    "with open(tmp_ds_txt_files[0], 'r') as file:\n",
    "    article = file.read()\n",
    "\n",
    "## Create a new row\n",
    "new_row = pd.DataFrame({'id': pdf_name[:-2], 'title': metadata_df[metadata_df['id'] == pdf_name[:-2]]['title'], 'abstract': metadata_df[metadata_df['id'] == pdf_name[:-2]]['abstract'].values[0], 'article': article}, index=[0])\n",
    "\n",
    "## Append row to the dataframe\n",
    "dataset_df = pd.concat([dataset_df, new_row], ignore_index=True)\n",
    "\n",
    "## Print the dataset dataframe\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2301.00002</td>\n",
       "      <td>evaluating alternative glyph design for showin...</td>\n",
       "      <td>we present experimental results to explore a...</td>\n",
       "      <td>introduction section for\\nsearch of \"largest\" ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "5  2301.00002  evaluating alternative glyph design for showin...   \n",
       "\n",
       "                                            abstract  \\\n",
       "5    we present experimental results to explore a...   \n",
       "\n",
       "                                             article  \n",
       "5  introduction section for\\nsearch of \"largest\" ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.drop_duplicates(subset=['id'], keep='last', inplace=True)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicates from the dataset\n",
    "dataset_df = dataset_df.drop_duplicates(subset=['id'])\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421592"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_files = glob('extracted_articles/**/*.txt', recursive=True)\n",
    "len(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'introduction section for\\nsearch of \"largest\" values by looking up \"yellow\" regions,\\nwithout attending to every single items of \"yellow\".\\nthe second concerns scalability to feature distances. here\\nfeature distance is meant to represent target-distractor similarity. it is not the absolute features (e.g., yellow) that direct\\nour attention towards the answer; rather, what determines\\nperformance is the result of a comparison between target\\n(yellow) and other data features (such as pink and orange)\\nin the scene (e.g., yellow is different from other colors and\\nthe yellow regions stand out) [8]. in other words, one must\\nalso look at feature distractors [14], [41], [42], whether or\\nnot they are heterogeneous, and that the efficiency of a\\nscene guidance will decline as a function of the degree of\\ndistractor variation [19], [24], [43]. while generally, subjective reports from experiment i indicate that lengthy color\\nand lengthy texture show the similar perceptual speed.\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n9\\n• exp ii.h2 (correspondence errors). more categorical feature\\n\\nof color in the separable pairs will reduce c-errors, when\\nparticipants will choose the correct exponent level.\\n• exp ii.h3 (user behavior). more categorical dimension in the\\nseparable feature-pairs will lead to optimal users\\' behaviors:\\ni.e., participants can quickly locate task-related regions for\\ntasks that demand looking among many vectors due to global\\nscene features.\\n\\nfig. 6: visual mapping using color and texture in experiment ii. from the top to bottom, colors and texture segments\\nare mapped to exponent values from the largest to the\\nsmallest. the three numbers next to the 7-level colormap are\\nthe rgb values. the numbers next to the texture columns\\nare the proportion of black-on-white for the last 7-level\\ntexture configuration.\\nperformance of texture may decline faster than color as\\nthe exponent range increases because our vision is not as\\nsensitive to luminance-variation as to hues. for example, at\\nthe exponent-range of 7 in figure 6, the differences between\\nyellow and pink could be more differentiable than the two\\ntop-level textures of different amount of black. in this\\nstudy, we expanded the data range from the single level in\\nexperiment i to five ranges ∈ [3, 7] to understand featurepair scalability to feature distances. the efficiency of color in\\nexperiment i could well arise because the range (of 4) was\\nnot large enough.\\nthe third concerns the density effects on color choices.\\nfigure 7 shows two densities and two colormaps (a categorical colormap from colorbrewer [36] and a segmented\\ncontinuous colormap by the number of exponents generated\\nfrom the extended blackbody colormap). for a feature to\\nactually guide attention, we can see from figure 7, the\\nboundary detection with these colormaps is associated with\\ndata density. unless the data density was reasonably high,\\ndetecting the boundaries using continuous colormaps (figures 7a, 7b) is harder than the colorbrewer colormaps\\n(figures 7c, 7d).\\n4.2\\n\\nmethod\\n\\n4.2.1 feature-pairs\\nwe used lengthy color, lengthy texture, and baseline\\nsplitvectors in experiment ii. these three visualizations\\nwere chosen because lengthy color and lengthy texture are\\namong the best feature-pairs from experiment i and because\\ncolor and texture are among the most separable features according to ware [10]. to introduce a \"distractor\" experience\\nto measure scalability to feature distances, we vary the data\\nrange from the 4 levels in experiment i to 3 − 7 levels in\\nexperiment ii (see mapping in figure 13, appendix c.)\\n4.2.2 hypotheses\\nwe had the following hypotheses:\\n• exp ii.h1 (accuracy). more categorical feature in the separable pairs will be more effective. we thus anticipate a\\nrank order of effectiveness from high to low: lengthy color,\\nlengthy texture, and splitvectors.\\n\\n4.2.3\\n\\ntasks\\n\\nparticipants performed three tasks in which they had to\\ncompare all vectors to obtain an answer.\\nexp ii. task 1 (search): visual search. a vector search\\nwithin 20 seconds (figure 8a). find the vector with magnitude\\nx within 20 seconds. the target vector was shown at the\\nbottom-right corner of the screen. participants were asked\\nto find this vector.\\nexp ii. task 2 (max): find maximum. an extreme value\\nsearch within 20 seconds (figure 8b). within 20 seconds, locate the point of maximum magnitude when the exponent is x . x\\nin the study was a number from 0 to the maximum exponent\\n(∈ [2, 6]). this was a global task requiring participants to\\nfind the extremum among many vectors.\\nexp ii. task 3 (numerosity): estimate the total\\nnumber of unique vector exponents (figure 8c). estimate the\\ntotal number of unique vector exponents in the entire vector field\\nwithin 2 seconds. data are randomly chosen and modified to\\nproduce the 3 to 7 range.\\n4.2.4\\n\\ntask choices\\n\\ntasks are use-inspired by real-world quantum physics data\\nanalyses. experiment i drilled down to a single or at most\\ntwo spins. but global tasks are also of quantum physicists\\'\\ninterests, such as those involving understanding the distributions of quantum spin magnitudes. practically, a spin\\nrepresents charge density or the measure of the probability\\nof an electron being present at an infinitesimal element of\\nspace surrounding any given point. this probability varies\\ndue to electron traveling from one grid point to another and\\nis often interpreted together with its neighbors. quantum\\nphysicists are thus interested in searches for regions, where\\nlocal regions are defined by spin magnitude and different\\nregions would correspond to changes in exponent. often\\nthe most interesting regions are also those with specific\\ncharge densities (task 1) or largest magnitudes (task 2) . the\\nregional task is related to learning the number of interesting\\nregions or magnitude exponent clusters (task 3).\\nperforming tasks was limited to 20 seconds as a pilot\\nstudy showed that it took participants about ∈ [15, 25]\\nseconds or on average about 20 seconds to finish search\\ntasks 1 and 2. also, preattentive processing when used for\\nscene guidance involving a group of similar objects are\\noften fast for viewers to see and increasing the number of\\nitems should not significantly impair the search time. from\\nthe practical side for the last experiment, participants who\\nwould want a perfect score could just spend time counting.\\nconstraining the time allowed us to measure the accuracy\\nwhen they may have to use the scene feature.\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n10\\n\\n(a) continuous colormap and high-density data\\n\\n(b) continuous colormap and low-density data\\n\\n(c) categorical colormap and high-density data\\n\\n(d) categorical colormap and low-density data\\n\\nfig. 7: density effects on color choices to justify the use of dense sampling and categorical colormap (c) in experiment ii.\\nthis example dataset shows two colormaps: ( segmented-continuous (a and b) and categorical (c and d) colormaps), at two\\ndifferent data densities. (a) and (c) show data with the raw density from the simulation results; (b) and (d) were produced by removing\\naround 70% vector glyphs. the boundaries between the data categories are more recognizable when the data are dense in\\n(a) and (c) (comparing the 1st column and the 2nd column). at the same density (comparing the 1st and 2nd row), the\\nboundaries between levels are easier to recognize when spin vectors are rendered using a categorical colormap of (c) and\\n(d). we thus use the raw dense and categorical colormaps (c) in experiment ii.\\n4.2.5 data choices\\ndata were first sampled using the same approach as experiment i, and no data is used repeatedly in this experiment.\\nwe then modified the exponent range from 3 to 7 for the\\nthree tasks by normalizing the data to the desired new data\\nrange.\\nprior literature used both synthetic data and real-world\\ndata to construct the data visualization as test scenarios, enabling tight control over the stimulus parameters (e.g., [44]).\\nmost of the synthetic data in literature were to replicate\\nreal-world data characteristics; and others were explained\\nin fictitious use scenarios. the goal was primarily to prevent\\npreconceived user knowledge about the domain-specific\\nattributes. as a result, the synthetic data strike the right balance between real-world uses and the data characteristics.\\nin our cases, replicating characteristics in quantum\\nphysics data was challenging and indeed impossible, since\\natom behaviors in high-dimensional space were largely\\nunknown and thus were not easily simulated. our approach\\nwas therefore to randomly sample quantum physics simulation results to capture domain-specific attributes and then\\nmodify the data to suit evaluation purposes. we showed\\nour data to our physicist collaborators to ensure their validity. we confirmed that these modifications preserved the\\ndomain-specific schema of a scene in terms of the domainspecific structures and complexity from real simulations.\\n\\nthese modifications represented less than 4% of overall data\\npoints in each scene. finally, it improves the reuse of our\\nstudy results.\\n4.2.6 empirical study design\\ndependent and independent variables. we used a withinsubject design with two independent variables of featurepair (three levels: baseline splitvectors, lengthy color, and\\nlengthy texture) and exponent range (five levels: 3 − 7). the\\ndependent variable was relative error. we did not measure\\ntime since all tasks were time-constrained.\\nparticipants performed 3 (feature-pairs) × 5 (magnituderanges) × 3 (repetitions) = 45 trials for the first two tasks.\\nthree repetitions were used to give participants enough\\ntime to develop strategies. for numerosity tasks, the\\ndesign runs 4 repetitions, resulting in 3 (feature-pairs) ×\\n5 (exponent-ranges) × 4 (repetitions) = 60 trials. each participant thus executed 45 + 45 + 60 = 150 trials. completing\\nall tasks took about 32 minutes.\\nself-reporting strategies. several human-computer interaction (hci) approaches can help observe users\\' behaviors.\\nanswering questions can assist us to determine not just\\nwhich technique is better but also the strategies humans\\nadopt. for example, cognitive walkthrough (ctw) measures whether or not the users\\' actions match the designers\\'\\npre-designed steps. here we predicted that participants\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n11\\n\\n4.3\\n\\nexperiment ii: results and discussion\\n\\nwe collected 810 data points per task for the first two\\ntasks of search and max and 1080 points for the third\\nnumerosity task.\\n\\n(a) search: find the vector with magnitude x . (x : 731,\\nanswer: the point marked by two yellow triangles.)\\n\\n(b) max: which point has the maximum magnitude when\\nthe exponent is x ? (x : 1, answer: the point marked by two\\nyellow triangles.)\\n\\n(c) numerosity (num): estimate the total number of\\nunique vector exponents of the entire vector field within 2\\nseconds. (answer: 7)\\n\\nfig. 8: experiment ii three task types. the callouts show the\\ntask-relevant feature-pair(s).\\n\\nwould use the global scene-features as guidance to accomplish tasks. we interviewed participants and asked them to\\nverbalize their visual observations in accomplishing tasks.\\n4.2.7\\n\\nparticipants\\n\\neighteen new participants (12 male and 6 female, mean\\nage = 23.8, and standard deviation = 4.94) of diverse\\nbackgrounds participated in the study (seven in computer\\nscience, four in computer engineering, two in information\\nsystems, three in engineering, one in business school, and\\none in physics).\\nprocedure, interaction, and environment were the same\\nas those in the experiment i.\\n\\n4.3.1 analysis approaches\\nfor search and max tasks, we measured relative error\\n(which was the percentage the reported value was away\\nfrom the ground truth and the same as that of experiment\\ni) with sas repeated measure. the last numerosity\\ntask used error rate which was the percentage of incorrect\\nanswers of all trials for each participant. we also used\\nthe same outlier removal methods to remove instances of\\ncorrespondence errors for search and max.\\n4.3.2 overview of study results\\ntable 3 and figure 10 show the summary statistics; and\\nall error bars again represent 95% confidence intervals. we\\nobserved a significant main effect of feature-pair type on all\\nthree tasks. for the first two tasks, the post-hoc analysis\\nrevealed that lengthy color and lengthy texture were in\\nthe same group, the most efficient one and that relative\\nerrors were statistically significantly lower than those of\\nthe splitvectors. lengthy color remained the most accurate\\npair for the numerosity tasks. exponent-range was only\\na significant main effect for numerosity, with power\\nranges 3 and 4 were significantly better than 5, which was\\nbetter than 6 and 7.\\n4.3.3 more categorical features of separable dimensions\\nimproved accuracy\\nwe were interested to see if we could observe significant\\nmain effects of categorical features in the separable pairs\\nin this experiment. here we did observe the significant\\nmain effect and confirmed our first hypothesis (h1) for\\nboth search and max: in the general trend, more separable lengthy color was more effective than lengthy texture\\nwhich was better than splitvectors, and lengthy color and\\nlengthy texture were in the same tukey group, when viewers were in the correct data sub-categories.\\nlengthy color led to the most accurate answers, and\\nsplitvectors was better than lengthy texture for numerosity task. this result can be explained by participants\\' behaviors - more than half the participants suggested they\\nsimply look for the longest cylinder from splitvectors since\\nthey know the numerical values in the test were continuous. this behavior deviated from our original purpose of\\ntesting the global estimate but did show two perspectives in\\nfavor of this work: (1) participants developed task-specific\\nstrategies during the experiment for efficiency; (2) 3d length\\nstill supported judging large and small and it was not as\\neffective as color perhaps due to ensemble perception from\\ncategorical features.\\n4.3.4 color categories of separable pairs reduced correspondence errors by a large margin\\nour second hypothesis h2 was also supported. we first\\ntested the number of correspondence errors in search\\nand max in the same way as in experiment i. these results\\nwhen combined with those in experiment i confirmed again\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n12\\n\\ntable 3: exp ii: summary statistics by tasks. the significant\\nmain effects and the high effect size are in bold and the\\nmedium effect size is in italic. effect size is cohen\\'s d\\nfor tasks search and max, and cramer\\'s v for task\\nnumerosity (num). post-hoc tukey grouping results\\nare reported for significant main effects, where > means\\nstatistically significantly better and enclosing parentheses\\nmean they belong to the same tukey group. here, lc :\\nlengthy color and lt : lengthy texture.\\ntask\\n\\nvariables\\n\\nsignificance\\n\\nes\\n\\nsearch\\n\\nfeature-pair\\n\\nf(2, 261) = 18.4, p < 0.0001\\n(lc , lt ) > splitvectors\\nf(4, 261) = 3.0, p = 0.20\\n\\n0.46\\n\\nf(2, 261) = 15.4, p < 0.0001\\n(lc , lt ) > splitvectors\\nf(4, 261) = 0.3, p = 0.87\\n\\n0.47\\n\\nχ2 = 63.2, p < 0.0001\\nlc > splitvectors > lt\\nχ2 = 47.4, p < 0.0001\\n(3, 4) > 5 > (6, 7)\\n\\n0.25\\n\\npower-range\\nmax\\n\\nfeature-pair\\npower-range\\n\\nnum\\n\\nfeature-pair\\npower-range\\n\\n0.86\\n\\n0.11\\n\\n0.35\\n\\nfig. 9: experiment ii (tasks search and max): all instances of correspondence errors by participant. again, the\\nlengthy color has the least instances of correspondence error\\nwhilst the lengthy texture had the most.\\n\\nthat the lengthy color reduced correspondence errors. for\\nsearch, there were only a single instance of correspondence error. 36 instances of correspondence errors came\\nfrom 14 participants (mean= 2.57, 95% cis=[2.1, 3.04])\\n(figure 9 top). another 59 instances for max came from\\n16 of 18 participants, mean= 3.68, 95% cis= [2.85, 4.51])\\n(figure 9 bottom).\\n4.3.5 compensating the cost of search in complex data\\nthrough preattentive scene feature\\nthe visualizations in our study contained hundreds of\\nitems from realistic uses. subjective behaviors through selfreport suggested that they adopted a sequential task-driven\\nviewing strategy to first obtain gross regional distribution\\nof task-relevant exponents. after this, a visual comparison\\nwithin the same exponent region were achieved. with these\\ntwo steps, judging large or small or perceiving quantities\\n\\nfig. 10: relative error for tasks search and max was the\\npercentage the reported value was away from the ground\\ntruth. error rate for numerosity was the percentage of\\nwrong answers of all trials for each participant. the vertical\\naxis shows the relative error or error rate. same letters\\nrepresent the same post-hoc analysis group. all error bars\\nrepresent 95% confidence intervals.\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\naccurately from separable variables would not use objectlevel information process.\\nmany participants commented on how the number\\nof powers in the data affected their effectiveness. for\\nlengthy texture, 10 participants remarked that it was difficult to differentiate adjacent powers when the total power\\nlevel is around 4-5 for lengthy texture. the white and black\\ntextures were very easy to perceive. all but two participants\\nagreed that lengthy color could perhaps support up to 6.\\nchung et al. [42] studied ordering effects and it would be\\nchallenging to compare ours to their results because their\\nvisual features were not shown as a scene but an isolated\\nfeature. more than half of the participants felt that effectiveness of lengthy lengthy was not affected by changing\\nthe number of powers, since they looked for the longest\\nouter cylinder to help find the answer. these results may\\nsuggest that subregion selection with lengthy texture can\\nperhaps be better designed with interfaces when the users\\ncan interactively select a texture level.\\n\\n5\\n\\ng eneral d iscussion\\n\\nwe discuss the results from both experiments and suggest\\nfuture directions.\\n5.1 separable dimensions with preattentive guidance\\nfor large-magnitude-range quantum physics spins\\nour first principle in glyph design is to follow the convention to use separable variable pairs [6], [10]. the results\\nof experiment i showed that separable dimensions could\\nachieve the same efficiency as direct linear visualizations for\\ncomp and was always more efficient than integral pairs.\\nfor these local-tasks, we didn\\'t observe significant error\\nreduction.\\nour second principle in glyph design is to include categorical features in separable pairs. the results from experiment ii studied the rank order of the separable pairs and\\nfound that they indeed improved accuracy for global tasks.\\nlengthy texture and splitvectors in both experiments led to\\nmore correspondence errors than lengthy color. achieving\\nintegrated numerical readings by combining two separable\\nvisual features at object level seems not necessary.\\nthe separable-dimension pairs of lengthy color and\\nlengthy texture worked because they were preattentive\\nscene features. our experiments show that viewers adopted\\na sequential task-driven viewing strategy based on a view\\nhierarchy: viewers first obtain global distributions of the\\nscene. then, a visual scrutiny is possible within a subregion.\\nalthough splitvectors are separable, visual search for length\\namong length would be unguided because both targets and\\ndistractors contained the same visual variable. the more\\nseparable, the easier it would be to guide the attention.\\nusing coloring to provide some initial regional division may\\nbe always better than not. texture (luminance) could achieve\\nsimilar accuracy and efficiency as long as the task-relevant\\nregions could be detected.\\n5.2\\n\\nfeature guidance vs. scene guidance\\n\\ntaking into account both study results, we think an important part of the answer to visualization design is guidance\\n\\n13\\n\\nof attention. it is guided to some objects or locations over\\nothers by two broad methods: feature guidance (seeing objects)\\nand scene guidance (seeing global structures).\\nfeature guidance refers to guidance by properties of the\\ntask-target as well as the distractors (leading to correspondence errors). these features are limited to a relatively small\\nsubset of visual dimensions: color, size, texture, orientation,\\nshape, blur or shininess and so on. these features have been\\nbroadly studied in 3d glyph design (see reviews by healey\\nand enns [25], borgo et al. [6], lie et al. [46], ropinski et\\nal. [22], and mcnabb and laramee [28]). take one more\\nexample from quantum physics simulation results, but with\\na different task of searching for the structural distributions\\nin the power of 3 in figure 11 will guide attention to either\\nthe fat cylinders (figure 11a) or the bright yellow color\\n(figure 11d, 11b) or the very dark texture (figure 11c),\\ndepending on the feature-pair types.\\nworking with quantum physicists, we have noticed that\\nthe structure and content of the scene strongly constrain the\\npossible location of meaningful structures, guided \"scene\\nguidance\" constraints [8], [47]. scientific data are not random and are typically structured. contextual and global\\nstructural influences can arise from different sources of\\nvisual information. if we return to the max search task in\\nfigure 11 again, we will note that the chunk of darker or\\nlighter texture patterns and colors on these regular contour\\nstructures strongly influence our quick detection. this is\\na structural and physical constraint that can be utilized\\neffectively by viewers. this observation coupled with the\\nempirical study results may suggest an interesting future\\nwork and hypothesis: adding scene structure guidance\\nwould speed up quantitative discrimination, improve the\\naccuracy of comparison tasks, and reduce the perceived\\ndata complexity.\\nanother structure acting as guidance is the size itself.\\nit was used by participants seeking to resolve the numerostiy tasks to look for the longest outside cylinders.\\nwe have showed several examples like figure 11, our\\ncollaborator suggested that the cylinder-bases of the same\\nsize with the redundant encoding (figure 11b) also helped\\nlocate and group glyphs belonging to the same magnitude.\\nthis observation agrees with the most recent literature that\\nguidance-by-size in 3d must take advantage of knowledge\\nof the layout of the scene [45].\\nthough feature guidance can be preattentive and features are detected within a fraction of a second, scene\\nguidance is probably just about as fast (though precise\\nexperiments have not been done and our experiment ii only\\nmerely shows this effect). scene \\'gist\\' can be extracted from\\ncomplex images after very brief exposures [47] [48]. this\\ndoesn\\'t mean that a viewer instantly knows, say, where the\\nanswer is located. however, with a fraction of a second\\'s\\nexposure, a viewer will know enough about the spatial layout of the scene to guide his or her attention towards vector\\ngroups in the regions of interest. for example, categorical\\ncolor becomes scene features since these colorful glyphs\\nwere perceived as a whole\\na future direction, and also an approach to understanding the efficiency and the effectiveness of scene guidance,\\nis to conduct an eye-tracking study to give viewers a flashview of our spatial structures and then let the viewer see the\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n14\\n\\n(a) lengthy lengthx feature-pair\\n\\n(b) lengthy color/lengthx feature-pair\\n\\n(c) lengthy texture feature-pair\\n\\n(d) lengthy color feature-pair\\n\\nfig. 11: contours of simulation data. size from this viewpoint can guide visual grouping and size in 3d must take advantage\\nof knowledge of the layout of the scene [45].\\ndisplay only in a narrow range around the point of fixation:\\ndoes this brief preview guide attention and the gaze effectively?\\nwork in vision and visualization [49], [50], [51], [52] domain\\nhas measured and correlated performance on the glance or\\nglobal structure formation. vision science discovered long\\nago that seeing global scene structures in medical imaging\\ndecision making guides experts\\' attention (experts always\\nknow where to look) [53] [54].\\n\\n5.3\\n\\nredundancy and ensemble graphical perception\\n\\nour results showed that adding categorical colors, in which\\nthe correspondence parts could be quickly discriminated, is\\nscalable to a large number of items. our result agrees with\\nthat of northelfer and gleicher [55]. they observed that\\nredundant encoding using color and shape could strengthen\\ngrouping when searching for targets from multiple objects.\\ntheir explanation was a race model [55]: for separable\\ndimensions, the performance of a glyph with the redundant\\nencoding might be dominated by the feature with greater\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\nefficiency. we did not find efficiency improvement - this\\nsuggested that the grouping is generally fast. so it might\\nnot be the redundancy itself that contributed to scene understanding.\\nanother possible theory is perhaps ensemble perception,\\ni.e., \"the visual system\\'s ability to extract summary statistical information from groups of similar objects - often\\nin a brief glance\" [40]. also ensemble features are best\\nrepresented using the categorical features. to model parallel\\nprocessing, the target contrast signal theory by buetti et\\nal. [24] may suit our scenario better. it describes more\\nspecific time estimate it takes to evaluate items in parallel.\\nin visualization, we just began to understand the ensemble\\naverages (e.g., chen [11] and alberts et al. [56]) but have\\nlimited understanding of ensemble visual encoding choices\\nto guide attention to optimize behaviors. we leave this to\\nfuture work.\\n5.4 use our results in visualization tools and limitations of our work\\nvisualization is used when the goal is to augment human\\ncapabilities in situations where the problems might not be\\nsufficiently defined for algorithms to communicate certain\\ninformation. one of our showcase areas is quantum physics.\\nwe believe that the design principle of prompting the addition of categorical features in bivariate glyphs would be\\nbroadly applicable to glyph design. also, application domains carrying similar data attributes could reuse of work.\\nour current study concerns bivariate data visualization in\\nwhich the bivariate variables are component parts of scalar\\nvariables.\\nour design could have been improved by following\\nadvanced tensor glyph design methods. both generic [57]\\nand domain-specific requirements for glyph designs\\n[37] [58] [59] have led to the summary of glyph properties\\n(e.g., invariant, uniqueness, continuity) to guide design and\\nto render 2d and 3d tensors. a logic step is to truly understand the quantum physics principles to combine data\\nattributes and human perception to improve our domainspecific solutions.\\none limitation of this work is that we measure only\\na subset of tasks crucial to showing structures and omitted all tasks relevant to orientation. however, one may\\nargue that the vectors naturally encode orientation. when\\norientation is considered, we could address the multiplechannel mappings in two ways. the first solution is to use\\nthe lengthy texture to encode the quantitative glyphs and\\ncolor to encode the orientation clusters. the second solution\\nis to treat magnitude and orientation as two data facets\\nand use multiple views to display them separately, with\\none view showing magnitude and the other for orientation\\n(using munzner\\'s multiform design recommendations [60]).\\nthe second limitation here was that our experiments were\\nlimited to a relatively small subset of visual dimensions:\\ncolor, texture, and size. a future direction would be to try\\nshapes and glyphs to produce novel and useful design.\\n\\n6\\n\\n15\\n\\nlengthy color was the most efficient and effective for both\\nlocal and global tasks. the categorical features enable effective complex scene inspections. our empirical study results\\nprovide the following recommendations for designing 3d\\nbivariate glyphs. .\\n• highly separable pairs can be used for quantitative\\ncomparisons as long as these glyphs could guide attention (i.e., category forming). we recommend using\\nlengthy color.\\n• texture-based glyphs (lengthy texture) that introduces\\nluminance variation will only be recommended when\\ntask-relevant structures can be isolated.\\n• integral and separable bivariate feature-pairs have the\\nsimilar accuracy when the tasks are local. when the\\nsearch tasks are more complex, introducing categorical\\nfeatures in the separable feature-pairs will lead to perceptually accurate glyphs.\\n• 3d glyph scene would shorten task completion time\\nby combing two glyph design factors: separability and\\nvisual guidance from categorical features.\\n• the\\nredundant encoding (lengthy color/lengthx )\\ngreatly improved on task completion time of integral\\ndimensions (lengthy lengthx ) by adding separable and\\npreattentive color features.\\n\\nacknowledgments\\nthe work is supported in part by nsf iis-1302755, nsf\\ncns-1531491, and nist-70nanb13h181. the user study\\nwas funded by nsf grants with the osu irb approval\\nnumber 2018b0080. non-user study design work was supported by grant from nist-70nanb13h181. the authors\\nwould like to thank katrina avery for her excellent editorial\\nsupport and all participants for their time and contributions.\\nany opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s)\\nand do not necessarily reflect the views of the national\\nscience foundation. certain commercial products are identified in this paper in order to specify the experimental\\nprocedure adequately. such identification is not intended\\nto imply recommendation or endorsement by the national\\ninstitute of standards and technology, nor is it intended to\\nimply that the products identified are necessarily the best\\navailable for the purpose.\\n\\nr eferences\\n[1]\\n\\n[2]\\n\\n[3]\\n\\nc onclusion\\n\\nour findings in general suggest that, as we hypothesized, distinguishable separable dimensions with preattentive categorical features perform better. the separable pair\\n\\n[4]\\n\\nj. fuchs, p. isenberg, a. bezerianos, and d. keim, \"a\\nsystematic review of experimental studies on data glyphs,\"\\nieee transactions on visualization and computer graphics,\\nvol. 23, no. 7, pp. 1863–1879, 2017. [online]. available:\\nhttp://doi.org/10.1109/tvcg.2016.2549018\\nc. ware, \"quantitative texton sequences for legible bivariate\\nmaps,\" ieee transactions on visualization and computer graphics,\\nvol. 15, no. 6, pp. 1523–1529, 2009. [online]. available:\\nhttps://doi.org/10.1109/tvcg.2009.175\\nh. zhao, g. w. bryant, w. griffin, j. e. terrill, and j. chen,\\n\"validation of splitvectors encoding for quantitative visualization\\nof large-magnitude-range vector fields,\" ieee transactions on\\nvisualization and computer graphics, vol. 23, no. 6, pp. 1691–\\n1705, 2017. [online]. available: https://doi.org/10.1109/tvcg.\\n2016.2539949\\nd. j. wineland, \"nobel lecture: superposition, entanglement,\\nand raising schrödinger\\'s cat,\" reviews of modern physics,\\nvol. 85, no. 3, p. 1103, 2013. [online]. available: https:\\n//doi.org/10.1103/revmodphys.85.1103\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n[5]\\n\\na. treisman and g. gelade, \"a feature-integration theory of\\nattention,\" cognitive psychology, vol. 12, no. 1, pp. 97–136, 1980.\\n[online]. available: http://doi.org/10.1016/0010-0285(80)90005-5\\n[6] r. borgo, j. kehrer, d. h. chung, e. maguire, r. s.\\nlaramee, h. hauser, m. o. ward, and m. chen, \"glyph-based\\nvisualization: foundations, design guidelines, techniques and\\napplications,\" eurographics state of the art reports, pp. 39–63,\\n2013. [online]. available: http://diglib.eg.org/handle/10.2312/\\nconf.eg2013.stars.039-063\\n[7] j. m. wolfe and i. s. utochkin, \"what is a preattentive feature?\"\\ncurrent opinion in psychology, vol. 29, pp. 19–26, 2019. [online].\\navailable: https://doi.org/10.1016/j.copsyc.2018.11.005\\n[8] j. m. wolfe, \"guided search 6.0: an updated model of visual\\nsearch,\" psychonomic bulletin & review, pp. 1–33, 2021. [online].\\navailable: https://doi.org/10.3758/s13423-020-01859-9\\n[9] d. ariely, \"seeing sets: representation by statistical properties,\"\\npsychological science, vol. 12, no. 2, pp. 157–162, 2001. [online].\\navailable: https://doi.org/10.1111/1467-9280.00327\\n[10] c. ware, information visualization: perception for design, 3rd ed.\\nelsevier, 2012. [online]. available: https://www.elsevier.com/\\nbooks/information-visualization/ware/978-0-12-381464-7\\n[11] z. chen, r. zhuang, x. wang, y. ren, and r. a. abrams,\\n\"ensemble perception without attention depends upon attentional\\ncontrol settings,\" attention, perception, & psychophysics, vol. 83, pp.\\n1240–1250, 2021. [online]. available: https://doi.org/10.3758/\\ns13414-020-02067-2\\n[12] t. sekimoto and i. motoyoshi, \"ensemble perception without\\nphenomenal awareness of elements,\" scientific reports, vol. 12,\\nno. 1, pp. 1–8, 2022.\\n[13] j. maule, c. witzel, and a. franklin, \"getting the gist of multiple\\nhues: metric and categorical effects on ensemble perception of\\nhue,\" j. opt. soc. am. a, vol. 31, no. 4, pp. a93–a102, apr\\n2014. [online]. available: http://www.osapublishing.org/josaa/\\nabstract.cfm?uri=josaa-31-4-a93\\n[14] t. urness, v. interrante, i. marusic, e. longmire, and\\nb. ganapathisubramani, \"effectively visualizing multi-valued\\nflow data using color and texture,\" ieee visualization, pp. 115–\\n121, 2003. [online]. available: https://doi.org/10.1109/visual.\\n2003.1250362\\n[15] w. r. garner and g. l. felfoldy, \"integrality of stimulus\\ndimensions in various types of information processing,\" cognitive\\npsychology, vol. 1, no. 3, pp. 225–241, 1970. [online]. available:\\nhttps://doi.org/10.1016/0010-0285(70)90016-2\\n[16] c. g. healey and j. t. enns, \"large datasets at a glance: combining\\ntextures and colors in scientific visualization,\" ieee transactions\\non visualization and computer graphics, vol. 5, no. 2, pp. 145–167,\\n1999. [online]. available: https://doi.org/10.1109/2945.773807\\n[17] c. g. healey, k. s. booth, and j. t. enns, \"visualizing\\nreal-time multivariate data using preattentive processing,\" acm\\ntransactions on modeling and computer simulation, vol. 5, no. 3,\\npp. 190–221, 1995. [online]. available: http://doi.org/10.1145/\\n217853.217855\\n[18] j. m. wolfe and t. s. horowitz, \"what attributes guide the\\ndeployment of visual attention and how do they do it?\" nature\\nreviews neuroscience, vol. 5, no. 6, pp. 1–7, 2004. [online].\\navailable: http://doi.org/10.1038/nrn1411\\n[19] j. duncan and g. w. humphreys, \"visual search and stimulus\\nsimilarity.\" psychological review, vol. 96, no. 3, p. 433, 1989.\\n[online]. available: https://doi.org/10.1037/0033-295x.96.3.433\\n[20] h. strobelt, d. oelke, b. c. kwon, t. schreck, and h. pfister,\\n\"guidelines for effective usage of text highlighting techniques,\"\\nieee transactions on visualization and computer graphics,\\nvol. 22, no. 1, pp. 489–498, 2016. [online]. available: https:\\n//doi.org/10.1109/tvcg.2015.2467759\\n[21] c. g. healey, k. s. booth, and j. t. enns, \"high-speed visual\\nestimation using preattentive processing,\" acm transactions on\\ncomputer-human interaction, vol. 3, no. 2, pp. 107–135, 1996.\\n[online]. available: http://doi.org/10.1145/230562.230563\\n[22] t. ropinski, s. oeltze, and b. preim, \"survey of glyph-based\\nvisualization techniques for spatial multivariate medical data,\"\\ncomputers & graphics, vol. 35, no. 2, pp. 392–401, 2011. [online].\\navailable: https://doi.org/10.1016/j.cag.2011.01.011\\n[23] a. treisman and s. gormican, \"feature analysis in early\\nvision: evidence from search asymmetries,\" psychological review,\\nvol. 95, no. 1, pp. 15–48, 1988. [online]. available: https:\\n//doi.org/10.1037/0033-295x.95.1.15\\n\\n16\\n\\n[24] s. buetti, j. xu, and a. lleras, \"predicting how color and\\nshape combine in the human visual system to direct attention,\"\\nscientific reports, vol. 9, no. 1, pp. 1–11, 2019. [online]. available:\\nhttps://doi.org/10.1038/s41598-019-56238-9\\n[25] c. g. healey and j. t. enns, \"attention and visual memory\\nin visualization and computer graphics,\" ieee transactions on\\nvisualization and computer graphics, vol. 18, no. 7, pp. 1170–1188,\\n2012. [online]. available: https://doi.org/10.1109/tvcg.2011.127\\n[26] t. c. callaghan, \"interference and dominance in texture\\nsegregation: hue, geometric form, and line orientation,\"\\nperception, & psychophysics, vol. 46, no. 4, pp. 299–311, 1989.\\n[online]. available: https://doi.org/10.3758/bf03204984\\n[27] j. fuchs, p. isenberg, a. bezerianos, and d. keim, \"a\\nsystematic review of experimental studies on data glyphs,\"\\nieee transactions on visualization and computer graphics,\\nvol. 23, no. 7, pp. 1863–1879, 2017. [online]. available:\\nhttps://doi.org/10.1109/tvcg.2016.2549018\\n[28] l. mcnabb and r. s. laramee, \"survey of surveys (sos)-mapping\\nthe landscape of survey papers in information visualization,\"\\ncomputer graphics forum, vol. 36, no. 3, pp. 589–617, 2017.\\n[online]. available: https://doi.org/10.1111/cgf.13212\\n[29] j. mackinlay, \"automating the design of graphical presentations\\nof relational information,\" acm transactions on graphics,\\nvol. 5, no. 2, pp. 110–141, 1986. [online]. available: https:\\n//doi.org/10.1145/22949.22950\\n[30] w. s. cleveland and r. mcgill, \"graphical perception:\\ntheory, experimentation, and application to the development of\\ngraphical methods,\" journal of the american statistical association,\\nvol. 79, no. 387, pp. 531–554, 1984. [online]. available:\\nhttps://doi.org/10.2307/2288400\\n[31] s. m. casner, \"task-analytic approach to the automated\\ndesign of graphic presentations,\" acm transactions on graphics,\\nvol. 10, no. 2, pp. 111–151, 1991. [online]. available: https:\\n//doi.org/10.1145/108360.108361\\n[32] c. m. mccoleman, f. yang, t. f. brady, and s. franconeri, \"rethinking the ranks of visual channels,\" ieee transactions on visualization and computer graphics, 2021.\\n[33] ç. demiralp, m. s. bernstein, and j. heer, \"learning perceptual\\nkernels for visualization design,\" ieee transactions on visualization\\nand computer graphics, vol. 20, no. 12, pp. 1933–1942, 2014.\\n[online]. available: https://doi.org/10.1109/tvcg.2014.2346978\\n[34] b. e. rogowitz and a. d. kalvin, \"the \"which blair project\":\\na quick visual method for evaluating perceptual color maps,\"\\nieee visualization, pp. 183–191, 2001. [online]. available:\\nhttps://doi.org/10.1109/visual.2001.964510\\n[35] j. p. o\\'shea, m. agrawala, and m. s. banks, \"the influence\\nof shape cues on the perception of lighting direction,\" journal\\nof vision, vol. 10, no. 12, pp. 1–21, 2010. [online]. available:\\nhttps://doi.org/10.1167/10.12.21\\n[36] m. harrower and c. a. brewer, \"colorbrewer.org: an online\\ntool for selecting colour schemes for maps,\" the cartographic\\njournal, vol. 40, no. 1, pp. 27–37, 2003. [online]. available:\\nhttps://doi.org/10.1002/9780470979587.ch34\\n[37] c. zhang, t. schultz, k. lawonn, e. eisemann, and a. vilanova,\\n\"glyph-based comparative visualization for diffusion tensor\\nfields,\" ieee transactions on visualization and computer graphics,\\nvol. 22, no. 1, pp. 797–806, 2016. [online]. available: https:\\n//doi.org/10.1109/tvcg.2015.2467435\\n[38] j. bertin, semiology of graphics: diagrams, networks, maps. university of wisconsin press, 1967.\\n[39] j. cohen, statistical power analysis for the behavioral sciences.\\nnew york: academic press, 1988. [online]. available: https:\\n//doi.org/10.4324/9780203771587\\n[40] d. whitney and a. yamanashi leib, \"ensemble perception,\" annual review of psychology, vol. 69, pp. 105–129, 2018. [online]. available: https://doi.org/10.1146/annurev-psych-010416-044232\\n[41] d. acevedo, j. chen, and d. h. laidlaw, \"modeling\\nperceptual dominance among visual cues in multilayered\\nicon-based scientific visualizations,\" ieee visualization posters,\\n2007. [online]. available: https://vis.cs.brown.edu/docs/pdf/\\nacevedo-2007-mpd.pdf\\n[42] d. h. chung, d. archambault, r. borgo, d. j. edwards,\\nr. s. laramee, and m. chen, \"how ordered is it? on the\\nperceptual orderability of visual channels,\" computer graphics\\nforum, vol. 35, no. 3, pp. 131–140, 2016. [online]. available:\\nhttps://doi.org/10.1111/cgf.12889\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n[43] a. lleras, z. wang, g. j. p. ng, k. ballew, j. xu,\\nand s. buetti, \"a target contrast signal theory of parallel\\nprocessing in goal-directed search,\" attention, perception, &\\npsychophysics, vol. 82, no. 2, pp. 394–425, 2020. [online]. available:\\nhttps://doi.org/10.3758/s13414-019-01928-9\\n[44] a. forsberg, j. chen, and d. h. laidlaw, \"comparing 3d vector\\nfield visualization methods: a user study,\" ieee transactions on\\nvisualization and computer graphics, vol. 15, no. 6, pp. 1219–1226,\\n2009. [online]. available: https://doi.org/10.1109/tvcg.2009.\\n126\\n[45] m. p. eckstein, k. koehler, l. e. welbourne, and e. akbas,\\n\"humans, but not deep neural networks, often miss giant targets\\nin scenes,\" current biology, vol. 27, 2017. [online]. available:\\nhttps://doi.org/10.1016/j.cub.2017.07.068\\n[46] a. e. lie, j. kehrer, and h. hauser, \"critical design and realization\\naspects of glyph-based 3d data visualization,\" proceedings of the\\nspring conference on computer graphics, pp. 19–26, 2009. [online].\\navailable: https://doi.org/10.1145/1980462.1980470\\n[47] i. biederman, \"on processing information from a glance\\nat a scene,\" acm siggraph workshop on user-oriented\\ndesign of interactive graphics systems, 1977. [online]. available:\\nhttps://doi.org/10.1145/1024273.1024283\\n[48] a. oliva, \"gist of the scene,\" neurobiology of attention,\\nvol. 696, no. 64, pp. 251–258, 2005. [online]. available:\\nhttps://doi.org/10.1016/b978-012375731-9/50045-8\\n[49] g. ryan, a. mosca, r. chang, and e. wu, \"at a glance:\\npixel approximate entropy as a measure of line chart\\ncomplexity,\" ieee transactions on visualization and computer\\ngraphics, vol. 25, no. 1, pp. 872–881, 2019. [online]. available:\\nhttps://doi.org/10.1109/tvcg.2018.2865264\\n[50] z. bylinskii, p. isola, c. bainbridge, a. torralba, and a. oliva,\\n\"intrinsic and extrinsic effects on image memorability,\" vision\\nresearch, vol. 116, pp. 165–178, 2015. [online]. available:\\nhttps://doi.org/10.1016/j.visres.2015.03.005\\n[51] m. a. borkin, a. a. vo, z. bylinskii, p. isola, s. sunkavalli,\\na. oliva, and h. pfister, \"what makes a visualization\\nmemorable?\" ieee transactions on visualization and computer\\ngraphics, vol. 19, no. 12, pp. 2306–2315, 2013. [online]. available:\\nhttps://doi.org/10.1109/tvcg.2013.234\\n[52] r. li and j. chen, \"toward a deep understanding of what makes\\na scientific visualization memorable,\" ieee vis and arxiv preprint,\\n2018. [online]. available: https://arxiv.org/abs/1808.00607\\n[53] h. l. kundel, c. f. nodine, e. f. conant, and s. p. weinstein,\\n\"holistic component of image perception in mammogram\\ninterpretation: gaze-tracking study,\" radiology, vol. 242, no. 2,\\npp. 396–402, 2007. [online]. available: https://doi.org/10.1148/\\nradiol.2422051997\\n[54] t. drew, m. l.-h. võ, and j. m. wolfe, \"the invisible gorilla strikes\\nagain: sustained inattentional blindness in expert observers,\"\\npsychological science, vol. 24, no. 9, pp. 1848–1853, 2013. [online].\\navailable: https://doi.org/10.1177/0956797613479386\\n[55] c. nothelfer, m. gleicher, and s. franconeri, \"redundant\\nencoding strengthens segmentation and grouping in visual\\ndisplays of data,\" journal of experimental psychology: human\\nperception and performance, vol. 43, no. 9, p. 1667, 2017. [online].\\navailable: https://doi.org/10.1037/xhp0000314\\n[56] d. albers, m. correll, and m. gleicher, \"task-driven evaluation\\nof aggregation in time series visualization,\" in proceedings of\\nthe sigchi conference on human factors in computing systems,\\n2014, pp. 551–560. [online]. available: https://doi.org/10.1145/\\n2556288.2557200\\n[57] t. gerrits, c. rössl, and h. theisel, \"glyphs for general secondorder 2d and 3d tensors,\" ieee transactions on visualization and\\ncomputer graphics, vol. 23, no. 1, pp. 980–989, 2017. [online].\\navailable: https://doi.org/10.1109/tvcg.2016.2598998\\n[58] h.-j. schulz, t. nocke, m. heitzler, and h. schumann, \"a design\\nspace of visualization tasks,\" ieee transactions on visualization and\\ncomputer graphics, vol. 19, no. 12, pp. 2366–2375, 2013. [online].\\navailable: http://doi.org/10.1109/tvcg.2013.120\\n[59] g. kindlmann and c.-f. westin, \"diffusion tensor visualization\\nwith glyph packing,\" ieee transactions on visualization and\\ncomputer graphics, vol. 12, no. 5, pp. 1329–1336, 2006. [online].\\navailable: https://doi.org/10.1109/tvcg.2006.134\\n[60] t. munzner, visualization analysis and design. a k peters\\nvisualization series. crc press, 2014. [online]. available:\\nhttps://doi.org/10.1201/b17511\\n\\n17\\n\\nhenan zhao was a phd student in department\\nof computer science and electrical engineering at university of maryland, baltimore county.\\nshe received b.e. degree in computer science\\nand information security from nankai university,\\nchina. her research interests include design and\\nevaluation of perceptually accurate visualization\\ntechniques. this work was conducted while she\\nwas visiting the ohio state university.\\n\\ngarnett bryant received his phd at indiana university in theoretical condensed matter physics.\\nafter research positions at washington state\\nuniversity, the national bureau of standards,\\nmcdonnell research labs and the army research laboratory, he has worked at the national institute of standards and technology\\n(nist) since 1994. he is directing the quantum processes and metrology group at nist\\nwith experimental and theoretical programs on\\nnanoscale, condensed matter systems for quantum information science and metrology. he is a fellow of the joint quantum institute of nist/university of maryland, a fellow of the american\\nphysical society and a member of the ieee. his theoretical research\\nprogram focuses on nanosystems, nanooptics and quantum science.\\n\\nwesley griffin received his phd degree in computer science from the university of maryland,\\nbaltimore county. he is a developer at stellar\\nscience. his research interests include real-time\\ngraphics and graphics hardware. he is a member of acm siggraph, the ieee and the ieee\\ncomputer society.\\n\\njudith e. terrill is a computer scientist and the\\nleader of the high performance computing and\\nvisualization group at the national institute of\\nstandards and technology. she is a member of\\nthe ieee computer society, the association for\\ncomputing machinery, and the association for\\nthe advancement of artificial intelligence.\\n\\njian chen is an associate professor in computer science and engineering at the ohio\\nstate university. she received her phd degree\\nin computer science from virginia tech, and\\nher ms degree in mechanical engineering | precision instrument from tianjin university | tsinghua university, china. she was a postdoctoral fellow at brown university and a visiting\\nresearcher at harvard university. her current\\nresearch interests include visual design, 3d interaction, and human-ai teaming.\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n18\\n\\nevaluating glyph design for showing large-magnitude-range\\nquantum spins\\nadditional material\\n\\nempirical study training documents, source code, study data, and results are online at https : //osf.io/4xcf 5/?viewo nly =\\n94123139df 9c4ac984a1e0df 811cd580.\\n\\na. background c olor\\nfig. 12 shows an example represented by lengthy texture with gray, white, and black background colors. gray background\\ncolor was selected for the experiments. we could observe that both white and black cylinders with lengthy texture encoding\\ncould be displayed more clearly in the gray background (fig. 12, left).\\n\\nb. v isual m apping for c olor and t exture in the lengthy color and lengthy texture pairs\\nfig. 6 shows the visual mapping using color and texture in experiment ii. the horizontal axis represents the exponent\\nrange ∈ [3, 7]. we selected those categorical colors from colorbrewer [36]. for texture, the percentage of black is mapped\\nto the exponent-range. examples with three different exponent-ranges of 3, 5, and 7 are shown in fig. 13, in which color\\nand texture are used for the visual mapping of study data.\\n\\nc. v isual f eatures and e xponent-r ange\\nfig. 13 shows examples for visual features and three exponent-ranges of 3, 5, and 7. the figures with the same exponentrange were generated using the same data and different visual features. the dataset used in this figure is for illustration\\npurpose only and does not necessarily reflect all image features used in the vector magnitude experiments.\\n\\nfig. 12: examples using different background colors: gray, white, and black. figures on the top row are magnified views\\nof region 1, marked by orange-box on the left image, and the bottom row shows region 2. with white background, the\\nwhite cylinders would be washed out (top right image). with black background, the black cylinders would be washed out\\n(bottom right image). in this study, the neutral stimulus-free gray background was chosen.\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n19\\n\\n(a) lengthy lengthy (splitvectors)\\n\\n(b) lengthy color\\n\\n(c) lengthy texture\\n\\nfig. 13: experiment ii: examples of selected exponent ranges of 3, 5, and 7 (from the second left to right). we could see that\\nthe pattern of magnitude distribution is more revealing by categorical colors than by texture glyphs. coloring may show\\nmore steps with large exponent ranges and also give us a better understanding of data distribution. for example, we could\\nquickly focus on the orange region.\\n\\n\\x0cjournal of latex class files, vol. 14, no. 8, august 2015\\n\\n20\\n\\nd. s patial p roximity\\nfigures 14 and 15 show spatial distributions of the identified targets (participants\\' answers) to the correct targets in\\nthe search and max tasks in experiment ii. here locations of the correct targets are translated to the origin (0, 0, 0).\\nparticipants\\' answers are depicted in green and each dot represents a trial. dots may overlap. dots in orange illustrate\\nsome of the nearest spins whose exponent values differ from the target (located at the origin). comparing the distribution\\nof participants\\' answers and the orange dot locations illustrates one of the key quantum physics data attributes: quantum\\nphysics data are discrete; and spatial proximity is not correlated with the spin magnitude proximity. for complex data like\\nthis, using the structural features (e.g., from color) in search will help them be more efficient and reduce errors.\\n\\n(a) lengthy color\\n\\n(b) lengthy texture\\n\\n(c) lengthy lengthy\\n\\nfig. 14: experiment ii: search task. the spatial proximity of the locations of the identified targets, to the ground truth,\\nfor all trials in the study. here the ground truth locations are translated to the origin (0, 0, 0). this task was timeconstrained. among the 810 trials (or 270 trials for each bivariate glyph type), participants completed 262 lengthy color, 261\\nlengthy texture, and 251 lengthy lengthy trials.\\n\\n(a) lengthy color\\n\\n(b) lengthy texture\\n\\n(c) lengthy lengthy\\n\\nfig. 15: experiment ii: max task. the spatial proximity of the locations of the identified targets, to the ground truth (centered\\nat the origin (0, 0, 0), for all trials in this task. the yellow dots show the closest points from other-than-target-exponent\\nregions. here the ground truth locations are translated to the origin (0, 0, 0). among the 810 trials, participants gave an\\nanswer to 270 trials for each bivariate glyph type. among each of these 270, participants completed 269 lengthy color, 269\\nlengthy texture, and 259 lengthy lengthy trials in total.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the extracted article\n",
    "with open(tmp_ds_txt_files[0], 'r') as file:\n",
    "    article = file.read()\n",
    "\n",
    "(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_from_file(file_path):\n",
    "    full_id = file_path.split('/')[-1].split('.')[0] + '.' + file_path.split('/')[-1].split('.')[1]\n",
    "    id_without_version = full_id.split('v')[0]\n",
    "    return id_without_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_ds_extracted_article/tmp_ds/2301/arxiv/arxiv/pdf/2301/2301.00002v1.txt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ds_txt_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(extract_id_from_file(tmp_ds_txt_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "km",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
