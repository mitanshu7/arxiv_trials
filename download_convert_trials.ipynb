{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 12:00:46,650 - arxivdata.config - WARNING: default output directory is /home/mitanshu/arxiv_trials/arxiv-data\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import multiprocessing\n",
    "from arxiv_public_datasets.arxiv_public_data.fulltext import convert_directory_parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "total_cpu = multiprocessing.cpu_count()\n",
    "print(total_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to download a folder from the bucket\n",
    "def download_folder_transfer_manager(bucket_name, bucket_folder_name, local_folder_path, workers=total_cpu, max_results=10000):\n",
    "    \"\"\"Downloads a folder from the bucket.\"\"\"\n",
    "\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "    ## Create the folder if it doesn't exist\n",
    "    create_folder(local_folder_path)\n",
    "\n",
    "    ## Create an anonymous client for the bucket\n",
    "    storage_client = Client.create_anonymous_client()\n",
    "\n",
    "    ## Get the bucket and list the blobs\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results, prefix=bucket_folder_name)]\n",
    "\n",
    "    results = transfer_manager.download_many_to_path(\n",
    "        bucket, blob_names, destination_directory=local_folder_path, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(blob_names, results):\n",
    "        # The results list is either `None` or an exception for each blob in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Downloaded {} to {}.\".format(name, local_folder_path + name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to delete the original pdfs after they are converted to txt files\n",
    "def delete_pdfs_safe(directory_path):\n",
    "\n",
    "    ## Get all pdf files\n",
    "    pdf_files = glob(f\"{directory_path}/**/*.pdf\", recursive=True)\n",
    "    \n",
    "    ## Get all txt files\n",
    "    txt_files = glob(f\"{directory_path}/**/*.txt\", recursive=True)\n",
    "    \n",
    "    ## Convert to a set for faster searching\n",
    "    txt_files = set(txt_files)\n",
    "    \n",
    "\n",
    "    ## Remove pdf only if there is a corresponding txt file\n",
    "    for pdf in pdf_files:\n",
    "\n",
    "        # print('pdf file: ', pdf)\n",
    "        \n",
    "        ## Get the pdf name\n",
    "        # pdf_name = pdf.split('/')[-1].split('.')[0] + '.' + pdf.split('/')[-1].split('.')[1]\n",
    "        # print('pdf name: ', pdf_name)\n",
    "        \n",
    "        ## Get the txt name\n",
    "        txt_name = pdf.replace('.pdf', '.txt')\n",
    "        # print('txt name: ', txt_name)\n",
    "        \n",
    "        ## Check if txt file exists\n",
    "        if txt_name in txt_files:\n",
    "            \n",
    "            ## Remove the pdf\n",
    "            os.remove(pdf)\n",
    "            # print('Removed: ', pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder_path = 'tmp_ds/2301'\n",
    "download_folder_transfer_manager(bucket_name='arxiv-dataset', bucket_folder_name='arxiv/arxiv/pdf/2301', local_folder_path=local_folder_path, max_results=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = glob('scientific_dataset_*/', recursive=True)\n",
    "for data_folder in data_folders:\n",
    "    delete_pdfs_safe(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting:  scientific_dataset_2020/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 15:44:03,991 - arxivdata.fulltext - INFO: Searching \"scientific_dataset_2020/**/*.pdf\"...\n",
      "2024-04-06 15:44:03,991 - arxivdata.fulltext - INFO: Found: 1 pdfs\n",
      "2024-04-06 15:48:04,073 - arxivdata.fulltext - ERROR: Conversion failed for 'scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf': Command '['pdf2txt.py', '-o', 'scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf2txt', 'scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf']' timed out after 120 seconds\n",
      "2024-04-06 15:48:04,074 - arxivdata.fulltext - ERROR: File conversion failed for scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf: Conversion failed for 'scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf': Command '['pdf2txt.py', '-o', 'scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf2txt', 'scientific_dataset_2020/2001/arxiv/arxiv/pdf/2001/2001.00016v1.pdf']' timed out after 120 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed:  scientific_dataset_2020/\n",
      "name 'delete_pdfs_safe' is not defined\n",
      "Converting:  scientific_dataset_2021/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 15:48:04,602 - arxivdata.fulltext - INFO: Searching \"scientific_dataset_2021/**/*.pdf\"...\n",
      "2024-04-06 15:48:04,603 - arxivdata.fulltext - INFO: Found: 1 pdfs\n",
      "2024-04-06 15:48:14,262 - arxivdata.fulltext - ERROR: Conversion failed for 'scientific_dataset_2021/2108/arxiv/arxiv/pdf/2108/2108.05543v1.pdf': No accurate text could be extracted from \"scientific_dataset_2021/2108/arxiv/arxiv/pdf/2108/2108.05543v1.pdf\"\n",
      "2024-04-06 15:48:14,264 - arxivdata.fulltext - ERROR: File conversion failed for scientific_dataset_2021/2108/arxiv/arxiv/pdf/2108/2108.05543v1.pdf: Conversion failed for 'scientific_dataset_2021/2108/arxiv/arxiv/pdf/2108/2108.05543v1.pdf': No accurate text could be extracted from \"scientific_dataset_2021/2108/arxiv/arxiv/pdf/2108/2108.05543v1.pdf\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed:  scientific_dataset_2021/\n",
      "name 'delete_pdfs_safe' is not defined\n",
      "Converting:  scientific_dataset_2023/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 15:48:14,663 - arxivdata.fulltext - INFO: Searching \"scientific_dataset_2023/**/*.pdf\"...\n",
      "2024-04-06 15:48:14,663 - arxivdata.fulltext - INFO: Found: 5 pdfs\n",
      "2024-04-06 15:50:22,949 - arxivdata.fulltext - INFO: Searching \"scientific_dataset_2022/**/*.pdf\"...\n",
      "2024-04-06 15:50:22,949 - arxivdata.fulltext - INFO: Found: 0 pdfs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed:  scientific_dataset_2023/\n",
      "name 'delete_pdfs_safe' is not defined\n",
      "Converting:  scientific_dataset_2022/\n",
      "Failed:  scientific_dataset_2022/\n",
      "name 'delete_pdfs_safe' is not defined\n"
     ]
    }
   ],
   "source": [
    "data_folders = glob('scientific_dataset_*/', recursive=True)\n",
    "for data_folder in data_folders:\n",
    "\n",
    "    try:\n",
    "        print('Converting: ', data_folder)\n",
    "        convert_directory_parallel(data_folder, total_cpu)\n",
    "        delete_pdfs_safe(data_folder)\n",
    "        print('Done: ', data_folder)\n",
    "    except Exception as e:\n",
    "        print('Failed: ', data_folder)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_pdfs_safe(local_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_pdfs_safe('scientific_dataset_2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining pdfs:  2\n"
     ]
    }
   ],
   "source": [
    "remaining_pdfs = glob('scientific_dataset_*/**/*.pdf', recursive=True)\n",
    "print('Remaining pdfs: ', len(remaining_pdfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, _ = os.path.splitext(remaining_pdfs[0])\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_many_blobs_with_transfer_manager(\n",
    "    bucket_name, blob_names, destination_directory=\"\", workers=8\n",
    "):\n",
    "    \"\"\"Download blobs in a list by name, concurrently in a process pool.\n",
    "\n",
    "    The filename of each blob once downloaded is derived from the blob name and\n",
    "    the `destination_directory `parameter. For complete control of the filename\n",
    "    of each blob, use transfer_manager.download_many() instead.\n",
    "\n",
    "    Directories will be created automatically as needed to accommodate blob\n",
    "    names that include slashes.\n",
    "    \"\"\"\n",
    "\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The list of blob names to download. The names of each blobs will also\n",
    "    # be the name of each destination file (use transfer_manager.download_many()\n",
    "    # instead to control each destination file name). If there is a \"/\" in the\n",
    "    # blob name, then corresponding directories will be created on download.\n",
    "    # blob_names = [\"myblob\", \"myblob2\"]\n",
    "\n",
    "    # The directory on your computer to which to download all of the files. This\n",
    "    # string is prepended (with os.path.join()) to the name of each blob to form\n",
    "    # the full path. Relative paths and absolute paths are both accepted. An\n",
    "    # empty string means \"the current working directory\". Note that this\n",
    "    # parameter allows accepts directory traversal (\"../\" etc.) and is not\n",
    "    # intended for unsanitized end user input.\n",
    "    # destination_directory = \"\"\n",
    "\n",
    "    # The maximum number of processes to use for the operation. The performance\n",
    "    # impact of this value depends on the use case, but smaller files usually\n",
    "    # benefit from a higher number of processes. Each additional process occupies\n",
    "    # some CPU and memory resources until finished. Threads can be used instead\n",
    "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "    # workers=8\n",
    "\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "    storage_client = Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    results = transfer_manager.download_many_to_path(\n",
    "        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(blob_names, results):\n",
    "        # The results list is either `None` or an exception for each blob in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.storage import Client, transfer_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bucket_with_transfer_manager(\n",
    "    bucket_name, destination_directory=\"\", workers=8, max_results=1000\n",
    "):\n",
    "    \"\"\"Download all of the blobs in a bucket, concurrently in a process pool.\n",
    "\n",
    "    The filename of each blob once downloaded is derived from the blob name and\n",
    "    the `destination_directory `parameter. For complete control of the filename\n",
    "    of each blob, use transfer_manager.download_many() instead.\n",
    "\n",
    "    Directories will be created automatically as needed, for instance to\n",
    "    accommodate blob names that include slashes.\n",
    "    \"\"\"\n",
    "\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The directory on your computer to which to download all of the files. This\n",
    "    # string is prepended (with os.path.join()) to the name of each blob to form\n",
    "    # the full path. Relative paths and absolute paths are both accepted. An\n",
    "    # empty string means \"the current working directory\". Note that this\n",
    "    # parameter allows accepts directory traversal (\"../\" etc.) and is not\n",
    "    # intended for unsanitized end user input.\n",
    "    # destination_directory = \"\"\n",
    "\n",
    "    # The maximum number of processes to use for the operation. The performance\n",
    "    # impact of this value depends on the use case, but smaller files usually\n",
    "    # benefit from a higher number of processes. Each additional process occupies\n",
    "    # some CPU and memory resources until finished. Threads can be used instead\n",
    "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "    # workers=8\n",
    "\n",
    "    # The maximum number of results to fetch from bucket.list_blobs(). This\n",
    "    # sample code fetches all of the blobs up to max_results and queues them all\n",
    "    # for download at once. Though they will still be executed in batches up to\n",
    "    # the processes limit, queueing them all at once can be taxing on system\n",
    "    # memory if buckets are very large. Adjust max_results as needed for your\n",
    "    # system environment, or set it to None if you are sure the bucket is not\n",
    "    # too large to hold in memory easily.\n",
    "    # max_results=1000\n",
    "\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "    storage_client = Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n",
    "\n",
    "    results = transfer_manager.download_many_to_path(\n",
    "        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(blob_names, results):\n",
    "        # The results list is either `None` or an exception for each blob in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_folder_transfer_manager(bucket_name, bucket_folder_name, local_folder_path, workers=8, max_results=10000):\n",
    "    \"\"\"Downloads a folder from the bucket.\"\"\"\n",
    "\n",
    "    ## Create the folder if it doesn't exist\n",
    "    create_folder(local_folder_path)\n",
    "\n",
    "    ## Create an anonymous client for the bucket\n",
    "    storage_client = storage.Client.create_anonymous_client()\n",
    "\n",
    "    ## Get the bucket and list the blobs\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results, prefix=bucket_folder_name)]\n",
    "\n",
    "    results = storage.transfer_manager.download_many_to_path(\n",
    "        bucket, blob_names, destination_directory=local_folder_path, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(blob_names, results):\n",
    "        # The results list is either `None` or an exception for each blob in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Downloaded {} to {}.\".format(name, local_folder_path + name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_folder_counted(bucket_name, folder_name, local_folder_path, count):\n",
    "    \"\"\"Downloads a folder from the bucket.\"\"\"\n",
    "\n",
    "    ## Create the folder if it doesn't exist\n",
    "    create_folder(local_folder_path)\n",
    "\n",
    "    ## Create an anonymous client for the bucket\n",
    "    storage_client = storage.Client.create_anonymous_client()\n",
    "\n",
    "    ## Get the bucket and list the blobs\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=folder_name)\n",
    "\n",
    "    ## Initiate the counter\n",
    "    tmp_count = 0\n",
    "\n",
    "    ## Download the blobs\n",
    "    for blob in blobs:\n",
    "        \n",
    "        ## Check if the counter is less than the count\n",
    "        if tmp_count < count:\n",
    "\n",
    "          ## Increment the counter\n",
    "          tmp_count +=1\n",
    "\n",
    "          ## Download the file\n",
    "          filename = blob.name.replace('/', '_')\n",
    "          blob.download_to_filename(f\"{local_folder_path}/{filename}\")\n",
    "        ## If the counter is greater than the count, break the loop\n",
    "        else:\n",
    "          break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to delete the original pdfs after they are converted to txt files\n",
    "def delete_pdfs(directory_path):\n",
    "\n",
    "    ## Get all pdf files\n",
    "    pdf_files = glob(f\"{directory_path}/*.pdf\")\n",
    "    \n",
    "    ## Get all txt files\n",
    "    txt_files = glob(f\"{directory_path}/*.txt\")\n",
    "    \n",
    "    ## Convert to a set for faster searching\n",
    "    txt_files = set(txt_files)\n",
    "    \n",
    "\n",
    "    ## Remove pdf only if there is a corresponding txt file\n",
    "    for pdf in pdf_files:\n",
    "        \n",
    "        ## Get the pdf name\n",
    "        pdf_name = pdf.split('/')[-1].split('.')[0] + '.' + pdf.split('/')[-1].split('.')[1]\n",
    "        \n",
    "        ## Get the txt name\n",
    "        txt_name = f\"{directory_path}/{pdf_name}.txt\"\n",
    "        \n",
    "        ## Check if txt file exists\n",
    "        if txt_name in txt_files:\n",
    "            \n",
    "            ## Remove the pdf\n",
    "            os.remove(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder_transfer_manager(bucket_name='arxiv-dataset', bucket_folder_name=f'arxiv/arxiv/pdf/2301', local_folder_path='tmp_ds', max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Creating a list for the year and month\n",
    "# yymm_list = np.arange(start=2301, stop=2313, step=1)\n",
    "# yymm_list = [str(i) for i in yymm_list]\n",
    "# print(yymm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## loop to download the files, convert them to text and delete the pdfs\n",
    "# for yymm in yymm_list:\n",
    "\n",
    "#     local_folder_path = 'scientific_dataset_2023/{yymm}'\n",
    "    \n",
    "#     ## Download all the pdfs published on Arxiv in the year 20yy and month mm\n",
    "#     download_folder(bucket_name='arxiv-dataset', folder_name=f'arxiv/arxiv/pdf/{yymm}', local_folder_path=local_folder_path)\n",
    "\n",
    "#     ## Convert all the pdfs in the yymm directory to text\n",
    "#     convert_directory_parallel(local_folder_path, total_cpu)\n",
    "\n",
    "#     ## Delete them pdfs if they have been converted to txts\n",
    "#     delete_pdfs(local_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "## Function to delete the original pdfs after they are converted to txt files\n",
    "def delete_pdfs(directory_path):\n",
    "\n",
    "    ## Get all pdf files\n",
    "    pdf_files = glob(f\"{directory_path}/**/*.pdf\", recursive=True)\n",
    "    \n",
    "    ## Get all txt files\n",
    "    txt_files = glob(f\"{directory_path}/**/*.txt\", recursive=True)\n",
    "    \n",
    "    ## Convert to a set for faster searching\n",
    "    txt_files = set(txt_files)\n",
    "    \n",
    "\n",
    "    ## Remove pdf only if there is a corresponding txt file\n",
    "    for pdf in pdf_files:\n",
    "\n",
    "        print(pdf)\n",
    "        \n",
    "        ## Get the pdf name\n",
    "        pdf_name = pdf.split('/')[-1].split('.')[0] + '.' + pdf.split('/')[-1].split('.')[1]\n",
    "        # print(pdf_name)\n",
    "        \n",
    "        ## Get the txt name\n",
    "        txt_name = f\"{directory_path}/{pdf_name}.txt\"\n",
    "        print(txt_name)\n",
    "        \n",
    "        ## Check if txt file exists\n",
    "        if txt_name in txt_files:\n",
    "            \n",
    "            ## Remove the pdf\n",
    "            # os.remove(pdf)\n",
    "            print('Removed: ', pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'scientific_dataset_2023/2301'\n",
    "delete_pdfs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
